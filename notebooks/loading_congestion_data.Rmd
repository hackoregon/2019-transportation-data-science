---
title: "Loading the TriMet Congestion Data"
output: html_notebook
---

First, we define some functions:
```{r}

# load one of the trimet_stop_event CSV files
#' load_csv
#'
#' @param path path to a TriMet "trimet_stop_event" CSV file
#'
#' @return a tibble with the data from the file, with some added columns

load_csv <- function(path) {
  temp <- read_csv(
    path,

    # we use characters for most of the columns because it makes filtering in the "View" windows easier
    col_types = cols(
      .default = col_character(),
      SERVICE_DATE = col_date(format = "%d%b%Y:%H:%M:%S"),
      LEAVE_TIME = col_integer(),
      ROUTE_NUMBER = col_integer(),
      STOP_TIME = col_integer(),
      ARRIVE_TIME = col_integer(),
      DWELL = col_integer(),
      DOOR = col_integer(),
      LIFT= col_integer(), 
      ONS= col_integer(),
      OFFS= col_integer(),
      ESTIMATED_LOAD = col_integer(),
      MAXIMUM_SPEED = col_double(),
      TRAIN_MILEAGE = col_double(),
      PATTERN_DISTANCE = col_double(),
      LOCATION_DISTANCE = col_double(),
      X_COORDINATE = col_double(),
      Y_COORDINATE = col_double()
    )
  ) %>%
    
    filter(
      LOCATION_ID != 0,
      ROUTE_NUMBER > 0,
      ROUTE_NUMBER <= 291,
      SERVICE_KEY == "W" |
        SERVICE_KEY == "S" |
        SERVICE_KEY == "U" |
        SERVICE_KEY == "X"
    ) %>% 
    
    # add some conveniences for visualization
    mutate(
      WEEKDAY = lubridate::wday(SERVICE_DATE),
      STOP_HOURS = round(STOP_TIME / 3600.0),
      ARRIVE_HOURS = round(ARRIVE_TIME / 3600.0),
      LEAVE_HOURS = round(LEAVE_TIME / 3600.0),
      TRIP_KEY = paste(
        SERVICE_DATE,
        VEHICLE_NUMBER,
        ROUTE_NUMBER,
        DIRECTION,
        TRIP_NUMBER,
        sep = "_"
      )
    )
  
  # convert the service data to a character string
  temp$SERVICE_DATE <- as.character(temp$SERVICE_DATE)
  return(temp)
}

#' Group by trips
#'
#' @param stop_events a "stop events" tibble
#'
#' @return the tibble grouped by trips
group_by_trips <- function(stop_events) {
  
  # sort first to get each trip in chronological order
  stop_events %>% arrange(
    VEHICLE_NUMBER,
    SERVICE_DATE,
    ARRIVE_TIME
  ) %>% 
    group_by(TRIP_KEY)
}

#' Compute lagged columns
#' Omce we have the data grouped by trips, we want to add a column for the previous location ID and the time from when the vehicle left there to when it arrived here
#'
#' @param stop_events a stop_events tibble
#'
#' @return the tibble with the new columns
#'
compute_lagged_columns <- function(stop_events) {
  stop_events %>% 
    mutate(
      SECONDS_LATE = ARRIVE_TIME - STOP_TIME,
      FROM_LOCATION = lag(LOCATION_ID),
      LEFT_THERE = lag(LEAVE_TIME),
      TRAVEL_SECONDS = ARRIVE_TIME - LEFT_THERE
   )
}
```

Make sure we have the tidyverse and sf:
```{r}
if (!require(tidyverse)) install.packages("tidyverse")
library(tidyverse)
if (!require(sf)) install.packages("sf")
library(sf)
```

Load the TriMet shapefiles:
```{r}
routes <- read_sf("../data/external/tm_routes/tm_routes.shp")
route_stops <- read_sf("../data/external/tm_route_stops/tm_route_stops.shp")
```

We load the CSVs in three steps so we can run a garbage collection after each one:
```{r}
trimet_stop_events <- 
  load_csv("../data/raw/trimet_stop_event 1-30SEP2017.csv")
gc(full = TRUE, verbose = TRUE)

trimet_stop_events <- trimet_stop_events %>% bind_rows(
 load_csv("../data/raw/trimet_stop_event 1-31OCT2017.csv")
)
gc(full = TRUE, verbose = TRUE)

trimet_stop_events <- trimet_stop_events %>% bind_rows(
 load_csv("../data/raw/trimet_stop_event 1-30NOV2017.csv")
)
gc(full = TRUE, verbose = TRUE)

```

Group the rows by trips:
```{r}
trimet_stop_events <- trimet_stop_events %>% 
  group_by_trips()
gc(full = TRUE, verbose = TRUE)
```

Compute the travel times between stops:
```{r}
trimet_stop_events <- trimet_stop_events %>%
  compute_lagged_columns() %>% 
  filter(
    !is.na(TRAVEL_SECONDS),
    TRAVEL_SECONDS > 0
  )
gc(full = TRUE, verbose = TRUE)
```

Now we have the finished raw data - collect the edges of the network with their travel times:
```{r}
edge_data <- trimet_stop_events %>% select(
  SERVICE_DATE,
  ROUTE_NUMBER,
  DIRECTION,
  SERVICE_KEY,
  FROM_LOCATION,
  LOCATION_ID,
  WEEKDAY,
  ARRIVE_HOURS,
  SECONDS_LATE,
  TRAVEL_SECONDS,
  TRIP_KEY
) %>% 
ungroup() %>% 
write_csv(path = "../data/interim/edge_data.csv")

```

Do a five-number summary of TRAVEL_SECONDS for each edge:
```{r}
edge_stats <- edge_data %>% 
  group_by(
    SERVICE_KEY,
    FROM_LOCATION,
    LOCATION_ID
  ) %>% 
  summarize(
    COUNT = n(),
    P05 = quantile(TRAVEL_SECONDS, probs = 0.05, names = FALSE),
    P25 = quantile(TRAVEL_SECONDS, probs = 0.25, names = FALSE),
    P50 = quantile(TRAVEL_SECONDS, probs = 0.50, names = FALSE),
    P75 = quantile(TRAVEL_SECONDS, probs = 0.75, names = FALSE),
    P95 = quantile(TRAVEL_SECONDS, probs = 0.95, names = FALSE)
  ) %>%
  filter(COUNT > 20)
gc(full = TRUE, verbose = TRUE)

```
