---
title: "Loading the TriMet Congestion Data"
output: html_notebook
---

First, we define some functions:
```{r}

# load one of the trimet_stop_event CSV files
#' load_csv
#'
#' @param path path to a TriMet "trimet_stop_event" CSV file
#'
#' @return a tibble with the data from the file, with some added columns

load_csv <- function(path) {
  temp <- read_csv(
    path,

    col_types = cols(
      SERVICE_DATE = col_date(format = "%d%b%Y:%H:%M:%S"),
      PATTERN_DISTANCE = col_double()
    )
  ) %>%

  # drop unused columns now!
  select(
    -BADGE,
    -MAXIMUM_SPEED
    -TRAIN_MILEAGE
    -PATTERN_DISTANCE
    -LOCATION_DISTANCE
    -X_COORDINATE
    -Y_COORDINATE
    -DATA_SOURCE
    -SCHEDULE_STATUS
  ) %>%   
  filter(
    LOCATION_ID != 0,
    ROUTE_NUMBER > 0,
    ROUTE_NUMBER <= 291,
    SERVICE_KEY == "W" |
      SERVICE_KEY == "S" |
      SERVICE_KEY == "U" |
      SERVICE_KEY == "X"
  ) 
  
  # convert the service data to a character string
  temp$SERVICE_DATE <- as.character(temp$SERVICE_DATE)
    
  # add some conveniences for visualization
  temp <- temp %>% mutate(
    WEEKDAY = lubridate::wday(SERVICE_DATE),
    STOP_HOURS = round(STOP_TIME / 3600.0),
    ARRIVE_HOURS = round(ARRIVE_TIME / 3600.0),
    LEAVE_HOURS = round(LEAVE_TIME / 3600.0),
    TRIP_KEY = paste(
      SERVICE_DATE,
      VEHICLE_NUMBER,
      ROUTE_NUMBER,
      DIRECTION,
      TRIP_NUMBER,
      sep = "_"
    )
  )
return(temp)
}

#' Group by trips
#'
#' @param stop_events a "stop events" tibble
#'
#' @return the tibble grouped by trips
group_by_trips <- function(stop_events) {
  
  # sort first to get each trip in chronological order
  stop_events %>% arrange(
    VEHICLE_NUMBER,
    SERVICE_DATE,
    ARRIVE_TIME
  ) %>% 
    group_by(TRIP_KEY)
}

#' Compute lagged columns
#' Omce we have the data grouped by trips, we want to add a column for the previous location ID and the time from when the vehicle left there to when it arrived here
#'
#' @param stop_events a stop_events tibble
#'
#' @return the tibble with the new columns
#'
compute_lagged_columns <- function(stop_events) {
  stop_events %>% 
    mutate(
      SECONDS_LATE = ARRIVE_TIME - STOP_TIME,
      FROM_LOCATION = lag(LOCATION_ID),
      LEFT_THERE = lag(LEAVE_TIME),
      TRAVEL_SECONDS = ARRIVE_TIME - LEFT_THERE
   ) %>% 
    filter(
      !is.na(TRAVEL_SECONDS),
      TRAVEL_SECONDS > 0
    )
}
```

Make sure we have the tidyverse and sf.
```{r}
if (!require(tidyverse)) install.packages("tidyverse")
library(tidyverse)
```

Many of our machines don't have enough RAM to deal with the whole dataset. It turns out there are no dependencies between the months so we can easily process one month at a time.
```{r}

month_table <- tribble(
  ~table_prefix, ~input_file,
  "m2017_09", "trimet_stop_event 1-30SEP2017.csv",
  "m2017_10", "trimet_stop_event 1-31OCT2017.csv",
  "m2017_11", "trimet_stop_event 1-30NOV2017.csv"
) 
```

Now we can do the edge data extraction.
```{r}

for (i in 1:nrow(month_table)) {

  trimet_stop_events <- load_csv(
    paste("../data/raw", month_table$input_file[i], sep = "/")
  )  
  gc(full = TRUE, verbose = TRUE)

  trimet_stop_events <- trimet_stop_events %>% 
  group_by_trips()
  gc(full = TRUE, verbose = TRUE)

  trimet_stop_events <- trimet_stop_events %>%
    compute_lagged_columns()
  gc(full = TRUE, verbose = TRUE)

  edge_data <- trimet_stop_events %>% select(
    SERVICE_DATE,
    ROUTE_NUMBER,
    DIRECTION,
    SERVICE_KEY,
    FROM_LOCATION,
    LOCATION_ID,
    WEEKDAY,
    ARRIVE_HOURS,
    SECONDS_LATE,
    TRAVEL_SECONDS,
    TRIP_KEY
  ) %>% 
  ungroup()

  colnames(edge_data) <- tolower(colnames(edge_data))
  write_csv(paste(
    "../data/interim",
    month_table$table_prefix[i],
    edge_data.csv,
    sep = "/"
  ))
}

```
